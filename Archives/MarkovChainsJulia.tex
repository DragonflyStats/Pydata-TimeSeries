Finite Markov Chains

Overview
Definitions
Simulation
Marginal Distributions
Irreducibility and Aperiodicity
Stationary Distributions
Ergodicity
Computing Expectations
Exercises
Solutions
Overview

Markov chains are one of the most useful classes of stochastic processes

Attributes:

simple, flexible and supported by many elegant theoretical results
valuable for building intuition about random dynamic models
very useful in their own right
You will find them in many of the workhorse models of economics and finance

In this lecture we review some of the theory of Markov chains

We will also introduce some of the great routines for working with Markov chains available in QuantEcon

Prerequisite knowledge is basic probability and linear algebra

Definitions

The following concepts are fundamental

Stochastic Matrices

A stochastic matrix (or Markov matrix) is an n×nn×n square matrix P=P[s,s′]P=P[s,s′] such that

each element P[s,s′]P[s,s′] is nonnegative, and
each row P[s,⋅]P[s,⋅] sums to one
(The square brackets notation for indices is unconventional but ties in well with the code below and helps us differentiate between indices and time subscripts)

Let S:={1,…,n}S:={1,…,n}

Each row P[s,⋅]P[s,⋅] can be regarded as a distribution (probability mass function) on SS

It is not difficult to check [1] that if PP is a stochastic matrix, then so is the kk-th power PkPk for all k∈Nk∈N

Markov Chains

There is a close connection between stochastic matrices and Markov chains

A Markov chain {Xt}{Xt} on SS is a stochastic process on SS that has the Markov property

This means that, for any date tt and any state s′∈Ss′∈S,

(1)
P{Xt+1=s′|Xt}=P{Xt+1=s′|Xt,Xt−1,…}
P{Xt+1=s′|Xt}=P{Xt+1=s′|Xt,Xt−1,…}
In other words, knowing the current state is enough to understand probabilities for future states

In particular, the dynamics of a Markov chain are fully determined by the set of values

P[s,s′]:=P{Xt+1=s′|Xt=s}(s,s′∈S)
P[s,s′]:=P{Xt+1=s′|Xt=s}(s,s′∈S)
By construction,

P[s,s′]P[s,s′] is the probability of going from ss to s′s′ in one unit of time (one step)
P[s,⋅]P[s,⋅] is the conditional distribution of Xt+1Xt+1 given Xt=sXt=s
It’s clear that PP is a stochastic matrix

Conversely, if we take a stochastic matrix PP, we can generate a Markov chain {Xt}{Xt} as follows:

draw X0X0 from some specified distribution

for t=0,1,…t=0,1,…,

draw Xt+1Xt+1 from P[Xt,⋅]P[Xt,⋅]
By construction, the resulting process satisfies (1)

Example 1

Consider a worker who, at any given time tt, is either unemployed (state 1) or employed (state 2)

Suppose that, over a one month period,

An employed worker loses her job and becomes unemployed with probability β∈(0,1)β∈(0,1)
An unemployed worker finds a job with probability α∈(0,1)α∈(0,1)
In terms of a stochastic matrix, this tells us that P[1,2]=αP[1,2]=α and P[2,1]=βP[2,1]=β, or

P=(1−αβα1−β)
P=(1−ααβ1−β)
Once we have the values αα and ββ, we can address a range of questions, such as

What is the average duration of unemployment?
Over the long-run, what fraction of time does a worker find herself unemployed?
Conditional on employment, what is the probability of becoming unemployed at least once over the next 12 months?
Etc.
We’ll cover such applications below

Example 2

Using US unemployment data, Hamilton [Ham05] estimated the stochastic matrix

P:=⎛⎝⎜0.9710.14500.0290.7780.50800.0770.492⎞⎠⎟
P:=(0.9710.02900.1450.7780.07700.5080.492)
where

the frequency is monthly
the first state represents “normal growth”
the second state represents “mild recession”
the third state represents “severe recession”
For example, the matrix tells us that when the state is normal growth, the state will again be normal growth next month with probability 0.97

In general, large values on the main diagonal indicate persistence in the process {Xt}{Xt}

This Markov process can also be represented as a directed graph, with edges labeled by transition probabilities

../_images/hamilton_graph.png
Here “ng” is normal growth, “mr” is mild recession, etc.

Simulation

One of the most natural ways to answer questions about Markov chains is to simulate them

(As usual, to approximate the probability of event EE, we can simulate many times and count the fraction of times that EE occurs)

Nice functionality for simulating Markov chains exists in QuantEcon

This is probably what you should use in applications, since it’s is efficient and bundled with lots of other useful routines for handling Markov chains

However, it’s also a good exercise to roll our own routines — let’s do that first and then come back to the methods in QuantEcon

Rolling our own

To simulate a Markov chain, we need its stochastic matrix PP and either an initial state or a probability distribution ψψ for initial state to be drawn from

The Markov chain is then constructed as discussed above. To repeat:

At time t=0t=0, the X0X0 is set to some fixed state or chosen from ψψ
At each subsequent time tt, the new state Xt+1Xt+1 is drawn from P[Xt,⋅]P[Xt,⋅]
In order to implement this simulation procedure, we need a method for generating draws from a discrete distributions

For this task we’ll use DiscreteRV from QuantEcon

julia> using QuantEcon

julia> psi = [0.1, 0.9];      # Probabilities over sample space {1, 2}

julia> d = DiscreteRV(psi);

julia> draw(d, 5)             # Generate 5 independent draws from psi
5-element Array{Int64,1}:
 1
 2
 2
 1
 2
We’ll write our code as a function that takes the following three arguments

A stochastic matrix P
An initial state init
A positive integer sample_size representing the length of the time series the function should return
using QuantEcon

function mc_sample_path(P; init=1, sample_size=1000)
    X = Array(Int64, sample_size) # allocate memory
    X[1] = init
    # === convert each row of P into a distribution === #
    n = size(P)[1]
    P_dist = [DiscreteRV(vec(P[i,:])) for i in 1:n]

    # === generate the sample path === #
    for t in 1:(sample_size - 1)
        X[t+1] = draw(P_dist[X[t]])
    end
    return X
end
Let’s see how it works using the small matrix

(2)
P:=(0.40.20.60.8)
P:=(0.40.60.20.8)
As we’ll see later, for a long series drawn from P, the fraction of the sample that takes value 1 will be about 0.25

If you run the following code you should get roughly that answer

julia> P = [0.4 0.6; 0.2 0.8]
2x2 Array{Float64,2}:
 0.4  0.6
 0.2  0.8

julia> X = mc_sample_path(P, sample_size=100000);

julia> println(mean(X .== 1))
0.25171
Using QuantEcon’s Routines

As discussed above, QuantEcon has very nice routines for handling Markov chains, inluding simulation

Here’s an illustration using the same P as the preceding example

julia> using QuantEcon

julia> mc = MarkovChain(P)
Discrete Markov Chain
stochastic matrix:
2x2 Array{Float64,2}:
 0.4  0.6
 0.2  0.8

julia> P = [.4 .6; .2 .8];

julia> X = mc_sample_path(mc, [0.5, 0.5], 100000);

julia> println(mean(X .== 1))  # Should be about 0.25
0.24883
Marginal Distributions

Suppose that

{Xt}{Xt} is a Markov chain with stochastic matrix PP
the distribution of XtXt is known to be ψtψt
What then is the distribution of Xt+1Xt+1, or, more generally, of Xt+mXt+m?

(Motivation for these questions is given below)

Solution

Let ψtψt be the distribution of XtXt for t=0,1,2,…t=0,1,2,…

Our first aim is to find ψt+1ψt+1 given ψtψt and PP

To begin, pick any s′∈Ss′∈S

Using the law of total probability, we can decompose the probability that Xt+1=s′Xt+1=s′ as follows:

P{Xt+1=s′}=∑s∈SP{Xt+1=s′|Xt=s}⋅P{Xt=s}
P{Xt+1=s′}=∑s∈SP{Xt+1=s′|Xt=s}⋅P{Xt=s}
In words, to get the probability of being at s′s′ tomorrow, we account for all ways this can happen and sum their probabilities

Rewriting this statement in terms of marginal and conditional probabilities gives

ψt+1[s′]=∑s∈SP[s,s′]ψt[s]
ψt+1[s′]=∑s∈SP[s,s′]ψt[s]
There are nn such equations, one for each s′∈Ss′∈S

If we think of ψt+1ψt+1 and ψtψt as row vectors (as is traditional), these nn equations are summarized by the matrix expression

(3)
ψt+1=ψtP
ψt+1=ψtP
In other words, to move the distribution forward one unit of time, we postmultiply by PP

By repeating this mm times we move forward mm steps into the future

Hence, iterating on (3), the expression ψt+m=ψtPmψt+m=ψtPm is also valid — here PmPm is the mm-th power of PP

As a special case, we see that if ψ0ψ0 is the initial distribution from which X0X0 is drawn, then ψ0Pmψ0Pm is the distribution of XmXm

This is very important, so let’s repeat it

(4)
X0∼ψ0⟹Xm∼ψ0Pm
X0∼ψ0⟹Xm∼ψ0Pm
and, more generally,

(5)
Xt∼ψt⟹Xt+m∼ψtPm
Xt∼ψt⟹Xt+m∼ψtPm
Multiple Step Transition Probabilities

We know that the probability of transitioning from ss to s′s′ in one step is P[s,s′]P[s,s′]

It turns out that that the probability of transitioning from ss to s′s′ in mm steps is Pm[s,s′]Pm[s,s′], the [s,s′][s,s′]-th element of the mm-th power of PP

To see why, consider again (5), but now with ψtψt putting all probability on state ss

If we regard ψtψt as a vector, it is a vector with 1 in the ss-th position and zero elsewhere

Inserting this into (5), we see that, conditional on Xt=sXt=s, the distribution of Xt+mXt+m is the ss-th row of PmPm

In particular

P{Xt+m=s′}=Pm[s,s′]=[s,s′]-th element of Pm
P{Xt+m=s′}=Pm[s,s′]=[s,s′]-th element of Pm
Example: Probability of Recession

Recall the stochastic matrix PP for recession and growth considered above

Suppose that the current state is unknown — perhaps statistics are available only at the end of the current month

We estimate the probability that the economy is in state ss to be ψ[s]ψ[s]

The probability of being in recession (state 1 or state 2) in 6 months time is given by the inner product

ψP6⋅⎛⎝⎜011⎞⎠⎟
ψP6⋅(011)
Example 2: Cross-Sectional Distributions

Recall our model of employment / unemployment dynamics for a given worker discussed above

Consider a large (i.e., tending to infinite) population of workers, each of whose lifetime experiences are described by the specified dynamics, independently of one another

Let ψψ be the current cross-sectional distribution over {1,2}{1,2}

For example, ψ[1]ψ[1] is the unemployment rate
The cross-sectional distribution records the fractions of workers employed and unemployed at a given moment

The same distribution also describes the fractions of a particular worker’s career spent being employed and unemployed, respectively

Irreducibility and Aperiodicity

Just about every theoretical treatment of Markov chains has some discussion of the concepts of irreducibility and aperiodicity

Let’s see what they’re about

Irreducibility

Let PP be a fixed stochastic matrix

Two states ss and s′s′ are said to communicate with each other if there exist positive integers jj and kk such that

Pj[s,s′]>0andPk[s′,s]>0
Pj[s,s′]>0andPk[s′,s]>0
In view of our discussion above, this means precisely that

state ss can be reached eventually from state s′s′, and
state s′s′ can be reached eventually from state ss
The stochastic matrix PP is called irreducible if all states communicate; that is, if ss and s′s′ communicate for all s,s′s,s′ in S×SS×S

For example, consider the following transition probabilities for wealth of a ficticious set of households

digraph irreducible_mc { rankdir=LR; "poor" -> "poor" [label = "0.9"]; "poor" -> "middle class" [label = "0.1"]; "middle class" -> "poor" [label = "0.4"]; "middle class" -> "middle class" [label = "0.4"]; "middle class" -> "rich" [label = "0.2"]; "rich" -> "poor" [label = "0.1"]; "rich" -> "middle class" [label = "0.1"]; "rich" -> "rich" [label = "0.8"]; }

We can translate this into a stochastic matrix, putting zeros where there’s no edge between nodes

P:=⎛⎝⎜0.90.40.10.10.40.100.20.8⎞⎠⎟
P:=(0.90.100.40.40.20.10.10.8)
It’s clear from the graph that this stochastic matrix is irreducible: we can reach any state from any other state eventually

We can also test this using QuantEcon’s MarkovChain class

julia> using QuantEcon

julia> P = [0.9 0.1 0.0; 0.4 0.4 0.2; 0.1 0.1 0.8];

julia> mc = MarkovChain(P)
Discrete Markov Chain
stochastic matrix:
3x3 Array{Float64,2}:
 0.9  0.1  0.0
 0.4  0.4  0.2
 0.8  0.1  0.1

julia> is_irreducible(mc)
true
Here’s a more pessimistic scenario, where the poor are poor forever

digraph irreducible_mc { rankdir=LR; "poor" -> "poor" [label = "1.0"]; "middle class" -> "poor" [label = "0.1"]; "middle class" -> "middle class" [label = "0.8"]; "middle class" -> "rich" [label = "0.1"]; "rich" -> "middle class" [label = "0.2"]; "rich" -> "rich" [label = "0.8"]; }

This stochastic matrix is not irreducible, since, for example, rich is not accessible from poor

Let’s confirm this

julia> using QuantEcon

julia> P = [1.0 0.0 0.0; 0.1 0.8 0.1; 0.0 0.2 0.8];

julia> mc = MarkovChain(P);

julia> is_irreducible(mc)
false
We can also determine the “communication classes”

julia> communication_classes(mc)
2-element Array{Array{Int64,1},1}:
 [1]
 [2,3]
It might be clear to you already that irreducibility is going to be important in terms of long run outcomes

For example, poverty is a life sentence in the second graph but not the first

We’ll come back to this a bit later

Aperiodicity

Loosely speaking, a Markov chain is called periodic if it cycles in a predictible way and aperiodic otherwise

Here’s a trivial example with three states

digraph irreducible_mc { rankdir=LR; "a" -> "b" [label = "1.0"]; "b" -> "c" [label = "1.0"]; "c" -> "a" [label = "1.0"]; }

The chain cycles with period 3:

julia> using QuantEcon

julia> P = [0 1 0; 0 0 1; 1 0 0];

julia> mc = MarkovChain(P);

julia> period(mc)
3
More formally, the period of a state ss is the greatest common divisor of the set of integers

D(s):={j≥1:Pj[s,s]>0}
D(s):={j≥1:Pj[s,s]>0}
In the last example, D(s)={3,6,9,…}D(s)={3,6,9,…} for every state ss, so the period is 3

A stochastic matrix is called aperiodic if the period of every state is 1, and periodic otherwise

For example, the stochastic matrix associated with the transition probabilities below is periodic because, for example, state aa has period 2

digraph irreducible_mc { rankdir=LR; "a" -> "b" [label = "1.0"]; "b" -> "c" [label = "0.5"]; "b" -> "a" [label = "0.5"]; "c" -> "b" [label = "0.5"]; "c" -> "d" [label = "0.5"]; "d" -> "c" [label = "1.0"]; }

We can confirm that the stochastic matrix is periodic as follows

julia> P = zeros(4, 4);

julia> P[1,2] = 1;

julia> P[2, 1] = P[2, 3] = 0.5;

julia> P[3, 2] = P[3, 4] = 0.5;

julia> P[4, 3] = 1;

julia> mc = MarkovChain(P);

julia> period(mc)
2

julia> is_aperiodic(mc)
false
Stationary Distributions

As seen in (3), we can shift probabilities forward one unit of time via postmultiplication by PP

Some distributions are invariant under this updating process — for example,

julia> P = [.4 .6; .2 .8];

julia> psi = [0.25, 0.75];

julia> psi'*P
1x2 Array{Float64,2}:
 0.25  0.75
Such distributions are called stationary, or invariant

Formally, a distribution ψ∗ψ∗ on SS is called stationary for PP if ψ∗=ψ∗Pψ∗=ψ∗P

From this equality we immediately get ψ∗=ψ∗Ptψ∗=ψ∗Pt for all tt

This tells us an important fact: If the distribution of X0X0 is a stationary distribution, then XtXt will have this same distribution for all tt

Hence stationary distributions have a natural interpretation as stochastic steady states — we’ll discuss this more in just a moment

Mathematically, a stationary distribution is just a fixed point of PP when PP is thought of as the map ψ↦ψPψ↦ψP from (row) vectors to (row) vectors

Theorem Every stochastic matrix PP has at least one stationary distribution

(We are assuming here that the state space SS is finite; if not more assumptions are required)

For a proof of this result you can apply Brouwer’s fixed point theorem, or see EDTC, theorem 4.3.5

There may in fact be many stationary distributions corresponding to a given stochastic matrix PP

For example, if PP is the identity matrix, then all distributions are stationary
Since stationary distributions are long run equilibria, to get uniqueness we require that initial conditions are not infinitely persistent

Infinite persistence of initial conditions occurs if certain regions of the state space cannot be accessed from other regions, which is the opposite of irreducibility

This gives some intuition for the following fundamental theorem

Theorem. If PP is both aperiodic and irreducible, then

PP has exactly one stationary distribution ψ∗ψ∗
For any initial distribution ψ0ψ0, we have ∥ψ0Pt−ψ∗∥→0∥ψ0Pt−ψ∗∥→0 as t→∞t→∞
For a proof, see, for example, theorem 5.2 of [Haggstrom02]

(Note that part 1 of the theorem requires only irreducibility, whereas part 2 requires both irreducibility and aperiodicity)

One easy sufficient condition for aperiodicity and irreducibility is that every element of PP is strictly positive

Try to convince yourself of this
Example

Recall our model of employment / unemployment dynamics for a given worker discussed above

Assuming α∈(0,1)α∈(0,1) and β∈(0,1)β∈(0,1), the uniform ergodicity condition is satisfied

Let ψ∗=(p,1−p)ψ∗=(p,1−p) be the stationary distribution, so that pp corresponds to unemployment (state 1)

Using ψ∗=ψ∗Pψ∗=ψ∗P and a bit of algebra yields

p=βα+β
p=βα+β
This is, in some sense, a steady state probability of unemployment — more on interpretation below

Not surprisingly it tends to zero as β→0β→0, and to one as α→0α→0

Calculating Stationary Distributions

As discussed above, a given Markov matrix PP can have many stationary distributions

That is, there can be many row vectors ψψ such that ψ=ψPψ=ψP

In fact if PP has two distinct stationary distributions ψ1,ψ2ψ1,ψ2 then it has infinitely many, since in this case, as you can verify,

ψ3:=λψ1+(1−λ)ψ2
ψ3:=λψ1+(1−λ)ψ2
is a stationary distribuiton for PP for any λ∈[0,1]λ∈[0,1]

If we restrict attention to the case where only one stationary distribution exists, one option for finding it is to try to solve the linear system ψ(In−P)=0ψ(In−P)=0 for ψψ, where InIn is the n×nn×n identity

But the zero vector solves this equation

Hence we need to impose the restriction that the solution must be a probability distribution

A suitable algorithm is implemented in QuantEcon — the next code block illustrates

using QuantEcon P = [.4 .6; .2 .8] mc = MarkovChain(P) println(mc_compute_stationary(mc))
The stationary distribution is unique

Convergence to Stationarity

Part 2 of the Markov chain convergence theorem stated above tells us that the distribution of XtXt converges to the stationary distribution regardless of where we start off

This adds considerable weight to our interpretation of ψ∗ψ∗ as a stochastic steady state

The convergence in the theorem is illustrated in the next figure

../_images/mc_convergence_plot.png
Here

PP is the stochastic matrix for recession and growth considered above
The highest red dot is an arbitrarily chosen initial probability distribution ψψ, represented as a vector in R3R3
The other red dots are the distributions ψPtψPt for t=1,2,…t=1,2,…
The black dot is ψ∗ψ∗
The code for the figure can be found in the QuantEcon applications library — you might like to try experimenting with different initial conditions

Ergodicity

Under irreducibility, yet another important result obtains: For all s∈Ss∈S,

(6)
1n∑t=1n1{Xt=s}→ψ∗[s]as n→∞
1n∑t=1n1{Xt=s}→ψ∗[s]as n→∞
Here

1{Xt=s}=11{Xt=s}=1 if Xt=sXt=s and zero otherwise
convergence is with probability one
the result does not depend on the distribution (or value) of X0X0
The result tells us that the fraction of time the chain spends at state ss converges to ψ∗[s]ψ∗[s] as time goes to infinity

This gives us another way to interpret the stationary distribution — provided that the convergence result in (6) is valid

The convergence in (6) is a special case of a law of large numbers result for Markov chains — see EDTC, section 4.3.4 for some additional information

Example

Recall our cross-sectional interpretation of the employment / unemployment model discussed above

Assume that α∈(0,1)α∈(0,1) and β∈(0,1)β∈(0,1), so that irreducibility and aperiodicity both hold

We saw that the stationary distribution is (p,1−p)(p,1−p), where

p=βα+β
p=βα+β
In the cross-sectional interpretation, this is the fraction of people unemployed

In view of our latest (ergodicity) result, it is also the fraction of time that a worker can expect to spend unemployed

Thus, in the long-run, cross-sectional averages for a population and time-series averages for a given person coincide

This is one interpretation of the notion of ergodicity

Computing Expectations

We are interested in computing expectations of the form

(7)
E[h(Xt)]
E[h(Xt)]
and conditional expectations such as

(8)
E[h(Xt+k)∣Xt=s]
E[h(Xt+k)∣Xt=s]
where

{Xt}{Xt} is a Markov chain generated by n×nn×n stochastic matrix PP
h:S→Rh:S→R is a given function
The unconditional expectation (7) is easy: We just need to sum over the distribution of XtXt

That is, we take

E[h(Xt)]=∑s∈S(ψPt)[s]h[s]
E[h(Xt)]=∑s∈S(ψPt)[s]h[s]
where ψψ is the distribution of X0X0

In this setting it’s traditional to regard hh as a column vector, in which case the expression can be rewritten more simply as

E[h(Xt)]=ψPth
E[h(Xt)]=ψPth
As for the conditional expectation (8), we need to sum over the conditional distribution of Xt+kXt+k given Xt=sXt=s

We already know that this is Pk[s,⋅]Pk[s,⋅], so

(9)
E[h(Xt+k)∣Xt=s]=(Pkh)[s]
E[h(Xt+k)∣Xt=s]=(Pkh)[s]
The vector PkhPkh stores the conditional expectation E[h(Xt+k)∣Xt=s]E[h(Xt+k)∣Xt=s] over all ss

Expectations of Geometric Sums

Sometimes we also want to compute expectations of a geometric sum, such as ∑tβth(Xt)∑tβth(Xt)

In view of the preceding discussion, this is

E[∑j=0∞βjh(Xt+j)∣Xt=s]=[(I−βP)−1h][s]
E[∑j=0∞βjh(Xt+j)∣Xt=s]=[(I−βP)−1h][s]
where

(I−βP)−1=I+βP+β2P2+⋯
(I−βP)−1=I+βP+β2P2+⋯
Premultiplication by (I−βP)−1(I−βP)−1 amounts to “applying the resolvent operator“

Exercises

Exercise 1

According to the discussion immediately above, if a worker’s employment dynamics obey the stochastic matrix

P=(1−αβα1−β)
P=(1−ααβ1−β)
with α∈(0,1)α∈(0,1) and β∈(0,1)β∈(0,1), then, in the long-run, the fraction of time spent unemployed will be

p:=βα+β
p:=βα+β
In other words, if {Xt}{Xt} represents the Markov chain for employment, then X¯n→pX¯n→p as n→∞n→∞, where

X¯n:=1n∑t=1n1{Xt=1}
X¯n:=1n∑t=1n1{Xt=1}
Your exercise is to illustrate this convergence

First,

generate one simulated time series {Xt}{Xt} of length 10,000, starting at X0=1X0=1
plot X¯n−pX¯n−p against nn, where pp is as defined above
Second, repeat the first step, but this time taking X0=2X0=2

In both cases, set α=β=0.1α=β=0.1

The result should look something like the following — modulo randomness, of course

../_images/mc_ex1_plot.png
(You don’t need to add the fancy touches to the graph—see the solution if you’re interested)

Exercise 2

A topic of interest for economics and many other disciplines is ranking

Let’s now consider one of the most practical and important ranking problems — the rank assigned to web pages by search engines

(Although the problem is motivated from outside of economics, there is in fact a deep connection between search ranking systems and prices in certain competitive equilibria — see [DLP13])

To understand the issue, consider the set of results returned by a query to a web search engine

For the user, it is desirable to

receive a large set of accurate matches
have the matches returned in order, where the order corresponds to some measure of “importance”
Ranking according to a measure of importance is the problem we now consider

The methodology developed to solve this problem by Google founders Larry Page and Sergey Brin is known as PageRank

To illustrate the idea, consider the following diagram

../_images/web_graph.png
Imagine that this is a miniature version of the WWW, with

each node representing a web page
each arrow representing the existence of a link from one page to another
Now let’s think about which pages are likely to be important, in the sense of being valuable to a search engine user

One possible criterion for importance of a page is the number of inbound links — an indication of popularity

By this measure, m and j are the most important pages, with 5 inbound links each

However, what if the pages linking to m, say, are not themselves important?

Thinking this way, it seems appropriate to weight the inbound nodes by relative importance

The PageRank algorithm does precisely this

A slightly simplified presentation that captures the basic idea is as follows

Letting jj be (the integer index of) a typical page and rjrj be its ranking, we set

rj=∑i∈Ljriℓi
rj=∑i∈Ljriℓi
where

ℓiℓi is the total number of outbound links from ii
LjLj is the set of all pages ii such that ii has a link to jj
This is a measure of the number of inbound links, weighted by their own ranking (and normalized by 1/ℓi1/ℓi)

There is, however, another interpretation, and it brings us back to Markov chains

Let PP be the matrix given by P[i,j]=1{i→j}/ℓiP[i,j]=1{i→j}/ℓi where 1{i→j}=11{i→j}=1 if ii has a link to jj and zero otherwise

The matrix PP is a stochastic matrix provided that each page has at least one link

With this definition of PP we have

rj=∑i∈Ljriℓi=∑all i1{i→j}riℓi=∑all iP[i,j]ri
rj=∑i∈Ljriℓi=∑all i1{i→j}riℓi=∑all iP[i,j]ri
Writing rr for the row vector of rankings, this becomes r=rPr=rP

Hence rr is the stationary distribution of the stochastic matrix PP

Let’s think of P[i,j]P[i,j] as the probability of “moving” from page ii to page jj

The value P[i,j]P[i,j] has the interpretation

P[i,j]=1/kP[i,j]=1/k if ii has kk outbound links, and jj is one of them
P[i,j]=0P[i,j]=0 if ii has no direct link to jj
Thus, motion from page to page is that of a web surfer who moves from one page to another by randomly clicking on one of the links on that page

Here “random” means that each link is selected with equal probability

Since rr is the stationary distribution of PP, assuming that the uniform ergodicity condition is valid, we can interpret rjrj as the fraction of time that a (very persistent) random surfer spends at page jj

Your exercise is to apply this ranking algorithm to the graph pictured above, and return the list of pages ordered by rank

The data for this graph is in the web_graph_data.txt file from the main repository — you can also view it here

There is a total of 14 nodes (i.e., web pages), the first named a and the last named n

A typical line from the file has the form

d -> h;
This should be interpreted as meaning that there exists a link from d to h

To parse this file and extract the relevant information, you can use regular expressions

The following code snippet provides a hint as to how you can go about this

julia> matchall(r"\w", "x +++ y ****** z")
3-element Array{SubString{UTF8String},1}:
 "x"
 "y"
 "z"

julia> matchall(r"\w", "a ^^ b &&& \$\$ c")
3-element Array{SubString{UTF8String},1}:
 "a"
 "b"
 "c"
When you solve for the ranking, you will find that the highest ranked node is in fact g, while the lowest is a

Exercise 3

In numerical work it is sometimes convenient to replace a continuous model with a discrete one

In particular, Markov chains are routinely generated as discrete approximations to AR(1) processes of the form

yt+1=ρyt+ut+1
yt+1=ρyt+ut+1
Here utut is assumed to be iid and N(0,σ2u)N(0,σu2)

The variance of the stationary probability distribution of {yt}{yt} is

σ2y:=σ2u1−ρ2
σy2:=σu21−ρ2
Tauchen’s method [Tau86] is the most common method for approximating this continuous state process with a finite state Markov chain

As a first step we choose

nn, the number of states for the discrete approximation
mm, an integer that parameterizes the width of the state space
Next we create a state space {x0,…,xn−1}⊂R{x0,…,xn−1}⊂R and a stochastic n×nn×n matrix PP such that

x0=−mσyx0=−mσy
xn−1=mσyxn−1=mσy
xi+1=xi+sxi+1=xi+s where s=(xn−1−x0)/(n−1)s=(xn−1−x0)/(n−1)
P[i,j]P[i,j] represents the probability of transitioning from xixi to xjxj
Let FF be the cumulative distribution function of the normal distribution N(0,σ2u)N(0,σu2)

The values P[i,j]P[i,j] are computed to approximate the AR(1) process — omitting the derivation, the rules are as follows:

If j=0j=0, then set
P[i,j]=P[i,0]=F(x0−ρxi+s/2)
P[i,j]=P[i,0]=F(x0−ρxi+s/2)
If j=n−1j=n−1, then set
P[i,j]=P[i,n−1]=1−F(xn−1−ρxi−s/2)
P[i,j]=P[i,n−1]=1−F(xn−1−ρxi−s/2)
Otherwise, set
P[i,j]=F(xj−ρxi+s/2)−F(xj−ρxi−s/2)
P[i,j]=F(xj−ρxi+s/2)−F(xj−ρxi−s/2)
The exercise is to write a function approx_markov(rho, sigma_u, m=3, n=7) that returns {x0,…,xn−1}⊂R{x0,…,xn−1}⊂R and n×nn×n matrix PP as described above

Solutions

Solution notebook

Footnotes

[1]	Hint: First show that if PP and QQ are stochastic matrices then so is their product — to check the row sums, try postmultiplying by a column vector of ones. Finally, argue that PnPn is a stochastic matrix using induction.
[2]	Hint: Premultiply by a row vector of ones
Next topic Orthogonal Projection and its Applications
Previous topic Linear Algebra
 Back to top
 Share
© Copyright 2014, John Stachurski and Thomas J. Sargent. Created using Sphinx 1.3.1. 
